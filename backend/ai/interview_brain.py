"""
Interview Brain - Multi-modal reasoning engine that coordinates all components.
The central intelligence hub for the interview assistant.
"""
import time
import threading
from dataclasses import dataclass, field
from typing import Optional, Dict, Any, List, Callable, Generator
from enum import Enum
import logging
import google.generativeai as genai

from .vision_context_manager import VisionContextManager, MergedContext
from .speech_listener import SpeechListener, TranscriptResult
from .question_detector import QuestionDetector, DetectedQuestion, QuestionCategory
from .language_selector import LanguageSelector, ResumeLanguageData

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BrainMode(Enum):
    """Operating modes for the Interview Brain."""
    IDLE = "idle"
    LISTENING = "listening"
    PROCESSING = "processing"
    ANSWERING = "answering"
    PAUSED = "paused"


class ResponseType(Enum):
    """Types of responses the brain can generate."""
    CODE_SOLUTION = "code_solution"
    BEHAVIORAL_ANSWER = "behavioral_answer"
    TECHNICAL_EXPLANATION = "technical_explanation"
    FOLLOW_UP_QUESTION = "follow_up_question"
    CLARIFICATION = "clarification"


@dataclass
class BrainResponse:
    """Response generated by the Interview Brain."""
    content: str
    response_type: ResponseType
    language: Optional[str]
    confidence: float
    sources_used: List[str]
    generation_time: float
    question_context: Optional[str] = None
    code_blocks: List[str] = field(default_factory=list)


class InterviewBrain:
    """
    Multi-modal Interview Brain - The central intelligence coordinator.
    
    Coordinates:
    - Screen capture → OCR → Vision Context
    - Speech → Transcript → Question Detection
    - Resume → Language Selection → Filtered Responses
    - All context → LLM → Streaming Answers
    
    Key Features:
    - Real-time question detection from screen + audio
    - Resume-aware language selection
    - Context prioritization (screen > audio > history)
    - Streaming response generation
    - Conflict resolution (screen wins)
    """
    
    # System prompt for interview responses
    SYSTEM_PROMPT = """You are an expert interview coach helping a candidate succeed in technical interviews.

CRITICAL RULES:
1. Answer as if YOU are the candidate being interviewed
2. Use first-person perspective ("I would...", "In my experience...")
3. Be concise but thorough
4. For coding questions:
   - Write clean, working code in {language}
   - Add brief comments explaining key logic
   - Consider edge cases
   - Mention time/space complexity when relevant
5. For behavioral questions:
   - Use the STAR format (Situation, Task, Action, Result)
   - Be specific with examples
6. For system design:
   - Start with requirements clarification
   - Draw out the architecture conceptually
   - Discuss trade-offs

CANDIDATE BACKGROUND:
{candidate_context}

CURRENT CONTEXT:
{current_context}
"""
    
    def __init__(self, api_key: str, model_name: str = "gemini-2.0-flash"):
        # Initialize Gemini
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        
        # Initialize components
        self.vision_context = VisionContextManager()
        self.speech_listener: Optional[SpeechListener] = None
        self.question_detector = QuestionDetector()
        self.language_selector = LanguageSelector()
        
        # State
        self._mode = BrainMode.IDLE
        self._current_question: Optional[DetectedQuestion] = None
        self._resume_loaded = False
        self._conversation_history: List[Dict[str, str]] = []
        
        # Threading
        self._lock = threading.Lock()
        self._processing = False
        
        # Callbacks
        self._on_question_detected: Optional[Callable[[DetectedQuestion], None]] = None
        self._on_response_ready: Optional[Callable[[BrainResponse], None]] = None
        self._on_mode_change: Optional[Callable[[BrainMode], None]] = None
        
        logger.info("InterviewBrain initialized")
    
    def setup_speech(self, device_index: Optional[int] = None, calibrate: bool = True):
        """Setup speech recognition component."""
        self.speech_listener = SpeechListener(device_index=device_index)
        
        # Wire up callbacks
        self.speech_listener.set_callbacks(
            on_transcript=self._handle_transcript,
            on_coding_question=self._handle_coding_question_audio
        )
        
        if calibrate:
            self.speech_listener.calibrate()
        
        logger.info("Speech recognition configured")
    
    def load_resume(self, resume_text: str) -> ResumeLanguageData:
        """Load and analyze resume for language selection."""
        data = self.language_selector.extract_from_resume(resume_text)
        self.vision_context.set_resume_context({
            'languages': list(data.languages.keys()),
            'skills': list(data.all_technologies),
            'primary_language': data.primary_language
        })
        self._resume_loaded = True
        logger.info(f"Resume loaded: primary language = {data.primary_language}")
        return data
    
    def set_callbacks(self,
                     on_question_detected: Optional[Callable[[DetectedQuestion], None]] = None,
                     on_response_ready: Optional[Callable[[BrainResponse], None]] = None,
                     on_mode_change: Optional[Callable[[BrainMode], None]] = None):
        """Set callback functions for brain events."""
        self._on_question_detected = on_question_detected
        self._on_response_ready = on_response_ready
        self._on_mode_change = on_mode_change
    
    def start(self):
        """Start the interview brain - begin listening and processing."""
        with self._lock:
            if self._mode != BrainMode.IDLE and self._mode != BrainMode.PAUSED:
                logger.warning(f"Cannot start from mode: {self._mode}")
                return
            
            self._set_mode(BrainMode.LISTENING)
            
            if self.speech_listener:
                self.speech_listener.start_listening()
            
            logger.info("Interview Brain started")
    
    def stop(self):
        """Stop the interview brain."""
        with self._lock:
            if self.speech_listener:
                self.speech_listener.stop_listening()
            
            self._set_mode(BrainMode.IDLE)
            logger.info("Interview Brain stopped")
    
    def pause(self):
        """Pause processing temporarily."""
        with self._lock:
            if self.speech_listener:
                self.speech_listener.stop_listening()
            self._set_mode(BrainMode.PAUSED)
    
    def resume(self):
        """Resume from paused state."""
        self.start()
    
    def _set_mode(self, mode: BrainMode):
        """Set brain mode and trigger callback."""
        self._mode = mode
        if self._on_mode_change:
            self._on_mode_change(mode)
    
    def add_screen_content(self, text: str, confidence: float = 1.0,
                          detected_code: Optional[str] = None,
                          detected_language: Optional[str] = None):
        """Add screen OCR content to context."""
        self.vision_context.add_screen_context(
            text, confidence, detected_code, detected_language
        )
        
        # Try to detect question from screen
        question = self.question_detector.detect_from_text(text, source="screen")
        if question and question.confidence > 0.6:
            self._handle_detected_question(question)
    
    def _handle_transcript(self, transcript: TranscriptResult):
        """Handle incoming speech transcript."""
        self.vision_context.add_audio_context(
            transcript.text, transcript.confidence
        )
        
        # Try to detect question from audio
        if transcript.is_question:
            question = self.question_detector.detect_from_text(
                transcript.text, source="audio"
            )
            if question:
                self._handle_detected_question(question)
    
    def _handle_coding_question_audio(self, transcript: TranscriptResult):
        """Handle detected coding question from audio."""
        question = self.question_detector.detect_from_text(
            transcript.text, source="audio"
        )
        if question:
            self._handle_detected_question(question)
    
    def _handle_detected_question(self, question: DetectedQuestion):
        """Handle a detected question - prepare for answering."""
        with self._lock:
            self._current_question = question
            
            if self._on_question_detected:
                self._on_question_detected(question)
            
            logger.info(f"Question detected: {question.category.value} - {question.text[:50]}...")
    
    def process_question(self, question_text: Optional[str] = None) -> Generator[str, None, None]:
        """
        Process a question and stream the response.
        
        Args:
            question_text: Optional explicit question text. If None, uses detected question.
            
        Yields:
            Streaming response chunks
        """
        with self._lock:
            if self._processing:
                yield "Still processing previous question..."
                return
            self._processing = True
            self._set_mode(BrainMode.PROCESSING)
        
        try:
            # Get question
            if question_text:
                detected = self.question_detector.detect_from_text(question_text, "manual")
            else:
                detected = self._current_question
            
            if not detected:
                yield "No question detected. Please provide a question."
                return
            
            # Get merged context
            merged_context = self.vision_context.get_merged_context(detected.text)
            
            # Select language
            target_language = self.language_selector.select_language(
                detected.text,
                detected.expected_language
            )
            
            # Build prompt
            prompt = self._build_prompt(detected, merged_context, target_language)
            
            # Stream response
            self._set_mode(BrainMode.ANSWERING)
            start_time = time.time()
            full_response = ""
            
            try:
                response = self.model.generate_content(prompt, stream=True)
                
                for chunk in response:
                    if chunk.text:
                        full_response += chunk.text
                        yield chunk.text
                        
            except Exception as e:
                logger.error(f"Generation error: {e}")
                yield f"\n\n[Error generating response: {str(e)}]"
            
            # Record in conversation history
            self._conversation_history.append({
                'role': 'question',
                'content': detected.text
            })
            self._conversation_history.append({
                'role': 'answer',
                'content': full_response
            })
            
            # Add to context for continuity
            self.vision_context.add_conversation_turn('interviewer', detected.text)
            self.vision_context.add_conversation_turn('candidate', full_response[:500])
            
            generation_time = time.time() - start_time
            logger.info(f"Response generated in {generation_time:.2f}s")
            
        finally:
            with self._lock:
                self._processing = False
                self._set_mode(BrainMode.LISTENING)
    
    def generate_response(self, question_text: str) -> BrainResponse:
        """
        Generate a complete response (non-streaming).
        
        Args:
            question_text: The question to answer
            
        Returns:
            BrainResponse with full content
        """
        start_time = time.time()
        chunks = list(self.process_question(question_text))
        full_response = "".join(chunks)
        
        # Detect code blocks
        import re
        code_blocks = re.findall(r'```[\w]*\n(.*?)```', full_response, re.DOTALL)
        
        # Get question info
        question = self._current_question
        
        return BrainResponse(
            content=full_response,
            response_type=self._determine_response_type(question),
            language=question.expected_language if question else None,
            confidence=question.confidence if question else 0.7,
            sources_used=['screen', 'audio', 'resume'] if self._resume_loaded else ['screen', 'audio'],
            generation_time=time.time() - start_time,
            question_context=question.text if question else question_text,
            code_blocks=code_blocks
        )
    
    def _build_prompt(self, question: DetectedQuestion, 
                     context: MergedContext, 
                     language: str) -> str:
        """Build the full prompt for the LLM."""
        # Get candidate context
        candidate_context = self.language_selector.get_context_for_llm()
        if not candidate_context:
            candidate_context = "No resume loaded - assume proficiency in common languages."
        
        # Build current context
        context_parts = []
        if context.primary_context:
            context_parts.append(f"Screen Content:\n{context.primary_context}")
        if context.supporting_context:
            context_parts.append(f"Additional Context:\n" + "\n".join(context.supporting_context[:3]))
        if context.detected_code:
            context_parts.append(f"Detected Code:\n```\n{context.detected_code}\n```")
        
        current_context = "\n\n".join(context_parts) if context_parts else "No additional context available."
        
        # Format system prompt
        system = self.SYSTEM_PROMPT.format(
            language=language,
            candidate_context=candidate_context,
            current_context=current_context
        )
        
        # Build question prompt
        question_prompt = f"""
QUESTION TYPE: {question.category.value}
DIFFICULTY: {question.difficulty.value}
PREFERRED LANGUAGE: {language}

QUESTION:
{question.text}

Please provide a comprehensive answer as the interview candidate.
"""
        
        return f"{system}\n\n{question_prompt}"
    
    def _determine_response_type(self, question: Optional[DetectedQuestion]) -> ResponseType:
        """Determine the type of response based on question category."""
        if not question:
            return ResponseType.TECHNICAL_EXPLANATION
        
        mapping = {
            QuestionCategory.CODING_ALGORITHM: ResponseType.CODE_SOLUTION,
            QuestionCategory.DATA_STRUCTURE: ResponseType.CODE_SOLUTION,
            QuestionCategory.BEHAVIORAL: ResponseType.BEHAVIORAL_ANSWER,
            QuestionCategory.TECHNICAL_CONCEPT: ResponseType.TECHNICAL_EXPLANATION,
            QuestionCategory.SYSTEM_DESIGN: ResponseType.TECHNICAL_EXPLANATION,
            QuestionCategory.SQL_DATABASE: ResponseType.CODE_SOLUTION,
        }
        
        return mapping.get(question.category, ResponseType.TECHNICAL_EXPLANATION)
    
    def get_current_context_summary(self) -> Dict[str, Any]:
        """Get a summary of current context state."""
        merged = self.vision_context.get_merged_context()
        
        return {
            'mode': self._mode.value,
            'current_question': self._current_question.text[:100] if self._current_question else None,
            'question_category': self._current_question.category.value if self._current_question else None,
            'context_sources': [s.value for s in merged.sources_used],
            'token_count': merged.token_count,
            'detected_language': merged.detected_language,
            'resume_loaded': self._resume_loaded,
            'conversation_turns': len(self._conversation_history)
        }
    
    def get_conversation_history(self) -> List[Dict[str, str]]:
        """Get the conversation history."""
        return self._conversation_history.copy()
    
    def clear_context(self):
        """Clear all context and reset state."""
        self.vision_context.clear_old_context(0)  # Clear all
        self.question_detector.clear_history()
        self._conversation_history.clear()
        self._current_question = None
        logger.info("Context cleared")
    
    def get_stats(self) -> Dict[str, Any]:
        """Get comprehensive statistics."""
        return {
            'brain_mode': self._mode.value,
            'resume_loaded': self._resume_loaded,
            'questions_detected': len(self.question_detector.get_recent_questions(3600)),
            'conversation_turns': len(self._conversation_history),
            'context_stats': self.vision_context.get_context_stats(),
            'speech_stats': self.speech_listener.get_stats() if self.speech_listener else None,
            'language_stats': self.language_selector.get_stats()
        }
    
    @property
    def mode(self) -> BrainMode:
        return self._mode
    
    @property
    def is_processing(self) -> bool:
        return self._processing


# Factory function
def create_interview_brain(api_key: str, 
                          model_name: str = "gemini-2.0-flash") -> InterviewBrain:
    """Create a configured InterviewBrain instance."""
    return InterviewBrain(api_key=api_key, model_name=model_name)
